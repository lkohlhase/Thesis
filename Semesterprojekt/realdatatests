import generate_data
import numpy as np
import mdp
import matplotlib.pyplot as plt
import random
import sklearn.cluster as cluster
import toydatatests
import pickle
import os
import dtw
import time
import math
from procedures import *

def second_attempt(): #1 34, 2 34, 3 34, 5 34, 6 34 
    for j in range(60):
        print(j+3)
        samplenum=j+3
        sample=6
        data=generate_data.readSample(sample)
        transitions=generate_data.segmentations['86 0'+str(sample)]
        timesteps=20
        numfeatures=3
        testdata = np.array([k[:samplenum] for k in data[:transitions[1]['upper']]])
        sfanode=mdp.nodes.SFANode()
        expansionnode=mdp.nodes.PolynomialExpansionNode(2)
        testdata=expansionnode.execute(testdata)
        print(testdata.shape)
        sfanode.train(testdata)
        sfanode.stop_training()
        feature_values=sfanode.execute(testdata,n=3)
        distmatrix=generate_data.simmatrix2(numfeatures,feature_values,timesteps)
        clusterer=cluster.SpectralClustering(n_clusters=2,affinity='precomputed')
        binarylist=clusterer.fit_predict(distmatrix)
        binarylist=generate_data.reorder(binarylist)
        boundary=generate_data.find_boundary(binarylist)
        print(boundary)
def third_attempt():


    for j in range(9):
        file = open('valuelog'+str(j+1)+'.txt', 'w')
        print('Looking at sample '+str(j+1))
        file.write('Looking at sample'+str(j+1)+'\n')
        transitions = generate_data.segmentations['86 0' + str(j+1)]
        file.write("We are expecting a boundary from "+str(transitions[0]['lower'])+' to '+str(transitions[0]['upper'])+" \n")

        for i in range(40):
            print('Condensed down to'+str(i+3))
            sample=j+1
            timesteps = 20
            numfeatures = 3
            reducenumber=i+3
            numclusters=2
            boundary,binarylist,feature_values=GetClustering(numclusters,numfeatures, reducenumber, sample, timesteps)
            file.write('Reducing from 62 to '+str(reducenumber)+'\n')
            file.write(str(boundary)+'\n')
        file.close()


def windowsizetest():
    tobesaved={}
    boundariesmiddle2=[]
    boundarieslower2=[]
    boundariesupper2=[]
    boundariesterrible2=[]
    plt.figure()
    numsamples=7
    for j in range(numsamples):
        boundariesmiddle2.append([])
        boundarieslower2.append([])
        boundariesupper2.append([])
        boundariesterrible2.append([])
        sample=j+3
        print('Working on sample '+str(sample))
        data = generate_data.readSample(sample)
        transitions = generate_data.segmentations['86 0' + str(sample)]
        transitions = [x['upper'] for x in transitions]
        testdatamiddle = np.array([k for k in data[:transitions[1]]])
        testdatalower = np.array([k for k in data[:(transitions[1]+transitions[0])/2]])
        testdataupper = np.array([k for k in data[:(transitions[1]+transitions[2])/2]])
        testdataterrible = np.array([k for k in data[:transitions[2]]])
        plt.subplot(numsamples, 4,j*4+1 )
        print('Working on the right windowsize')
        for i in range(5):
            boundariesmiddle2[-1].append([])
            reducenumber=10+i*5
            print('Reducing from 62 to '+str(reducenumber))
            clusteringdata=GetClusteringFromData(testdatamiddle,numclusters=2,numfeatures=3,reducenumber=reducenumber,timesteps=20)
            boundariesmiddle2[-1][-1].append(clusteringdata[0])
            boundariesmiddle=clusteringdata[0][0]
            plt.plot(boundariesmiddle,reducenumber,'x')
        plt.plot([0,0],[0,35])
        plt.plot([transitions[0],transitions[0]],[0,35],'b')
        plt.plot([transitions[1],transitions[1]],[0,35],'r')
        plt.subplot(numsamples,4,j*4+2)
        print('Working on a too small windowsize')
        for i in range(5):
            boundarieslower2[-1].append([])
            reducenumber=10+i*5
            print('Reducing from 62 to '+str(reducenumber))
            clusteringdata = GetClusteringFromData(testdatalower, numclusters=2, numfeatures=3, reducenumber=reducenumber, timesteps=20)
            boundarieslower2[-1][-1].append(clusteringdata[0])
            boundarieslower = clusteringdata[0][0]
            plt.plot(boundarieslower,reducenumber,'x')
        plt.plot([0,0],[0,35])
        plt.plot([transitions[0],transitions[0]],[0,35],'b')
        plt.plot([transitions[1],transitions[1]],[0,35],'r')
        plt.plot([(transitions[1]+transitions[0])/2,(transitions[1]+transitions[0])/2],[0,35],'g')
        plt.subplot(numsamples,4,j*4+3)
        print('Working on a too big windowsize')
        for i in range(5):
            boundariesupper2[-1].append([])
            reducenumber=10+i*5
            print('Reducing from 62 to '+str(reducenumber))
            clusteringdata = GetClusteringFromData(testdataupper, numclusters=2, numfeatures=3, reducenumber=reducenumber, timesteps=20)
            boundariesupper2[-1][-1].append(clusteringdata[0])
            boundariesupper = clusteringdata[0][0]
            plt.plot(boundariesupper,reducenumber,'x')
        plt.plot([0,0],[0,35])
        plt.plot([transitions[0],transitions[0]],[0,35],'b')
        plt.plot([transitions[1],transitions[1]],[0,35],'r')
        plt.plot([(transitions[1]+transitions[2])/2,(transitions[1]+transitions[2])/2],[0,35],'g')
        plt.subplot(numsamples,4,j*4+4)
        print('Working on a terrible windowsize')
        for i in range(5):
            boundariesterrible2[-1].append([])
            reducenumber=10+i*5
            print('Reducing from 62 to '+str(reducenumber))
            clusteringdata = GetClusteringFromData(testdataterrible, numclusters=2, numfeatures=3, reducenumber=reducenumber, timesteps=20)
            boundariesterrible2[-1][-1].append(clusteringdata[0])
            boundariesterrible = clusteringdata[0][0]
            plt.plot(boundariesterrible,reducenumber,'x')
        plt.plot([0,0],[0,35])
        plt.plot([transitions[0],transitions[0]],[0,35],'b')
        plt.plot([transitions[1],transitions[1]],[0,35],'r')
        plt.plot([(transitions[2]+transitions[2])/2,(transitions[2]+transitions[2])/2],[0,35],'g')
    plt.show()
    tobesaved['boundariesmiddle']=boundariesmiddle2
    tobesaved['boundariesupper']=boundariesupper2
    tobesaved['boundarieslower']=boundarieslower2
    tobesaved['boundariesterrible']=boundariesterrible2
    pickle.dump(tobesaved,open('Logs/windowsizetest','wb'))
def minibatchprettypicture():
    tobesaved=[['Transition points found by minibatch'],['Errors for those transitions']]
    counter=logcounter('prettypicturecounter')
    windowsize=[1200,2000,2000,2000,2000,1800,2000,1800,2000]
    boundaries=[]
    boundaries2=[]
    errors2=[]
    axis=[]
    numsamples=7
    #plt.figure()
    for j in range(numsamples):
        boundaries2.append([])
        errors2.append([])
        sample = j + 3
        #plt.subplot(numsamples,1,j+1)
        transitions = generate_data.segmentations['86 0' + str(sample)]
        boundaries.append([])
        axis.append([])
        print('Looking at sample'+str(sample)+'\n')
        for i in range(5):
            reducenumber=10+i*5
            print('Reducing from 62 to '+str(reducenumber))
            bndrys,errors=minibatchfromsample(sample, windowsize[sample - 1], reducenumber=reducenumber)
            boundaries[-1].append(bndrys)
            axis[-1].append([reducenumber for i in boundaries[-1][-1]])
            for k in range(len(bndrys)):
                value=50*(1-float(errors[k]))
                print('*'*50)
                print(value)
                #plt.scatter(bndrys[k],reducenumber,'x',s=value)
            boundaries2[-1].append(boundaries)
            errors2[-1].append(errors)
        transitionaxis=[x['middle'] for x in transitions]
        for point in transitionaxis:
            plt.plot([point,point],[0,35],'b')
    #plt.show()
    tobesaved.append(boundaries2)
    tobesaved.append(errors2)

    pickle.dump(tobesaved, open('Logs/prettypicture' + str(counter), 'wb'))


def minibatchtest():
    for i in range(4):
        print(10+i*5)
        print(minibatchfromsample(5, 1400, reducenumber=(10 + i * 5)))
def GetClustering(numclusters=2,numfeatures=3, reducenumber=10, sample=1, timesteps=20):
    '''
    Takes parameters and automatically finds boundary, featurevalues, and clustering for the first points from the CMU data set
    :param numclusters: How many clusters it makes
    :param numfeatures: Number of features to be used in vectors for similarity matrix
    :param reducenumber: number of dimensions to reduce data to
    :param sample: What sample we're looking at
    :param timesteps: Number of timesteps to be used in vectors for similarity matrix
    :return: feature values, list of clustering and boundary
    '''
    feature_values = GetSFAValues(numclusters,numfeatures, reducenumber, sample)
    distmatrix = generate_data.simmatrix2(numfeatures, feature_values, timesteps)
    clusterer=cluster.SpectralClustering(n_clusters=numclusters, affinity='precomputed')
    binarylist=clusterer.fit_predict(distmatrix)
    binarylist=generate_data.reorder(binarylist)
    if numclusters==2:
        boundary=generate_data.find_boundary(binarylist)
    else:
        boundary=generate_data.find_boundariesold(binarylist)
    return boundary,binarylist,feature_values


def minibatchfromsample(sample, steplength, numfeatures=3, reducenumber=10, timesteps=10, numclusters=2):
    print(reducenumber)
    data=generate_data.readSample(sample)
    transitions=generate_data.segmentations['86 0'+str(sample)]
    currentmiddleboundary=0
    boundaries=[]
    errors=[]
    while (currentmiddleboundary+steplength<len(data)):
        currentdata=data[currentmiddleboundary:currentmiddleboundary+steplength]
        currentdata=np.array([k for k in currentdata])
        boundary,binlist,featurevalues=GetClusteringFromData(currentdata,reducenumber=reducenumber)

        currentmiddleboundary+=boundary[0]
        errors.append(boundary[1])
        print(currentmiddleboundary)
        print(boundary )
        boundaries.append(currentmiddleboundary)
    return boundaries,errors

def GetSFAValuesFromData(data,numfeatures=3, reducenumber=10):
    testdata=pca_reduce(data,reducenumber)
    sfanode=mdp.nodes.SFANode()
    expansionnode = mdp.nodes.PolynomialExpansionNode(2)
    testdata = expansionnode.execute(testdata)
    print(testdata.shape)
    sfanode.train(testdata)
    sfanode.stop_training()
    feature_values = sfanode.execute(testdata, n=numfeatures)
    return feature_values

def GetClusteringFromData(data, numclusters=2,numfeatures=3, reducenumber=10, timesteps=20):
    '''
    Same as GetClustering, except inputs arbitrary data. Has to be formatted correctly though, so big np.array() with rows being data points
    '''
    featurevalues=GetSFAValuesFromData(data,numfeatures,reducenumber)
    distmatrix = generate_data.simmatrix2(numfeatures, featurevalues, timesteps)
    clusterer = cluster.SpectralClustering(n_clusters=numclusters, affinity='precomputed')
    binarylist = clusterer.fit_predict(distmatrix)
    binarylist = generate_data.reorder(binarylist)
    if numclusters == 2:
        boundary = generate_data.find_boundary(binarylist)
    else:
        boundary = generate_data.find_boundariesold(binarylist)
    return boundary, binarylist, featurevalues

def GetSFAValues(numclusters=2,numfeatures=3, reducenumber=10, sample=1):
    data = generate_data.readSample(sample)
    transitions = generate_data.segmentations['86 0' + str(sample)]
    testdata = np.array([k for k in data[:transitions[numclusters-1]['upper']]])
    testdata = pca_reduce(testdata, reducenumber)
    sfanode = mdp.nodes.SFANode()
    expansionnode = mdp.nodes.PolynomialExpansionNode(2)
    testdata = expansionnode.execute(testdata)
    sfanode.train(testdata)
    sfanode.stop_training()
    feature_values = sfanode.execute(testdata, n=numfeatures)
    return feature_values


def pca_reduce(testdata,numdimensions):
    '''
    Takes testdata in the form needed for mdp nodes and then uses pca to reduce it down to numdimensions
    '''
    pcanode=mdp.nodes.PCANode(output_dim=numdimensions)
    pcanode.train(testdata)
    pcanode.stop_training()
    return pcanode.execute(testdata)
def first_attempt():
    file=open('valuelog.txt' ,'w')

    for j in range(15):
        file.write('*'*50)
        file.write('New File here')
        file.write(str(j))
        for i in range(15):
            print(i+10)
            file.write(str(i+10))
            samplenum=j+1
            data=generate_data.readSample(samplenum)
            if samplenum<10:
                transitions=generate_data.segmentations['86 0'+str(samplenum)]
            else:
                transitions=generate_data.segmentations['86 '+str(samplenum)]
            timesteps=20
            numfeatures=3
            testdata=np.array([k[:10+i] for k in data[:transitions[1]['upper']]])
            sfanode=mdp.nodes.SFANode()
            expansionnode=mdp.nodes.PolynomialExpansionNode(2)
            testdata=expansionnode.execute(testdata)
            print(testdata.shape)
            file.write(str(testdata.shape))
            sfanode.train(testdata)
            sfanode.stop_training()
            feature_values=sfanode.execute(testdata,n=3)
            distmatrix=generate_data.simmatrix2(numfeatures,feature_values,timesteps)
            plt.matshow(distmatrix)
            clusterer=cluster.SpectralClustering(n_clusters=2,affinity='precomputed')
            binarylist=clusterer.fit_predict(distmatrix)
            binarylist=generate_data.reorder(binarylist)
            print(binarylist)
            boundary=generate_data.find_boundary(binarylist)
            print(boundary)
            file.write(str(boundary))


def manywindows():
    counter=logcounter('manywindowscounter')
    tobesaved=[['testdata','feature_values','binarylist']]
    num_cluster=10
    blksize=250
    testdata,boundaries=generate_data.toydata2(blocksize=blksize,numblocks=num_cluster,noise=0.0001,numfeatures=20)
    tobesaved.append(testdata)
    sfanode=mdp.nodes.SFANode()
    expansionnode = mdp.nodes.PolynomialExpansionNode(2)
    testdata2 = expansionnode.execute(testdata)

    print(testdata2.shape)
    sfanode.train(testdata2)
    sfanode.stop_training()
    feature_values = sfanode.execute(testdata2, n=5)
    tobesaved.append(feature_values)
    distmatrix = generate_data.simmatrix2(5, feature_values, 20)
    plt.matshow(distmatrix)
    plt.show()
    clusterer = cluster.SpectralClustering(n_clusters=num_cluster, affinity='precomputed')
    binarylist = clusterer.fit_predict(distmatrix)
    binarylist = generate_data.reorder(binarylist)
    tobesaved.append(binarylist)
    tobesaved.append(distmatrix)
    pickle.dump(tobesaved,open('Logs/manywindows'+str(counter),'wb'))
    plt.plot(range(len(binarylist)),binarylist,'x')
    for boundary in boundaries:
        plt.plot([boundary,boundary],[0,10])
    plt.show()
    # print(binarylist)
    # plt.figure(1)
    # plt.plot(range(len(binarylist)),binarylist,'x')
    # plt.figure(2)
    # toydatatests.show_features(testdata,feature_values)
    # plt.show()
    # print(minibatch(testdata,2*blksize,numfeatures=5))


def logcounter(filename):
    file = open('Logs/'+filename, 'r')
    counter = int(file.read())
    file.close()
    file = open('Logs/'+filename, 'w')
    file.write(str(counter + 1))
    file.close()
    return counter


def numfeaturesminibatch():
    num_cluster = 10
    noises=[0.0001,0.01,0.02,0.03,0.04,0.05,0.06]
    blksize = 500
    plt.figure()
    for j in range(len(noises)):
        testdata2, boundaries = generate_data.toydata2(blocksize=blksize, numblocks=num_cluster, noise=noises[j], numfeatures=30)
        plt.subplot(len(noises), 1, j + 1)
        plt.title('Noise: '+str(noises[j]))
        for i in range(5):
            numfeatures = 5 + i * 3
            testdata=np.array([k[:numfeatures] for k in testdata2])
            print('Working on noise '+str(noises[j]))
            asdf = minibatch(testdata, 2 * blksize, reducenumber=5)
            for i in boundaries:
                plt.plot([i,i],[0,30])
            for i in asdf:
                plt.plot(i,numfeatures,'x')
    plt.show()
def windowsize2():
    dic=pickle.load(open('Logs/windowsizetest','rb'))

    plt.figure()
    for i in range(7):
        sample = i + 3
        transitions = generate_data.segmentations['86 0' + str(sample)]
        transitions = [x['upper'] for x in transitions]
        plt.subplot(7,4,i*4+1)
        plt.plot([0, 0], [0, 35])
        plt.plot([transitions[0], transitions[0]], [0, 35], 'b')
        plt.plot([transitions[1], transitions[1]], [0, 35], 'r')
        for j in range(5):
            reducenumber=10+j*5
            boundary=dic['boundariesmiddle'][i][j]
            print(boundary[0][1])
            plt.scatter(boundary[0][0],reducenumber,marker='x',s=boundary[0][1]**2*1000+20)
        plt.subplot(7,4,i*4+2)
        plt.plot([0, 0], [0, 35])
        plt.plot([transitions[0], transitions[0]], [0, 35], 'b')
        plt.plot([transitions[1], transitions[1]], [0, 35], 'r')
        plt.plot([(transitions[1] + transitions[0]) / 2, (transitions[1] + transitions[0]) / 2], [0, 35], 'g')
        for j in range(5):
            reducenumber=10+j*5
            boundary=dic['boundarieslower'][i][j]
            print(boundary[0][1])
            plt.scatter(boundary[0][0],reducenumber,marker='x',s=boundary[0][1]**2*1000+20)
        plt.subplot(7,4,i*4+3)
        plt.plot([0, 0], [0, 35])
        plt.plot([transitions[0], transitions[0]], [0, 35], 'b')
        plt.plot([transitions[1], transitions[1]], [0, 35], 'r')
        plt.plot([(transitions[1] + transitions[2]) / 2, (transitions[1] + transitions[2]) / 2], [0, 35], 'g')
        for j in range(5):
            reducenumber=10+j*5
            boundary=dic['boundariesupper'][i][j]
            print(boundary[0][1])
            size=(1-boundary[0][1])**2*1000
            plt.scatter(boundary[0][0],reducenumber,marker='x',s=boundary[0][1]**2*1000+20)
        plt.subplot(7,4,i*4+4)
        plt.plot([0, 0], [0, 35])
        plt.plot([transitions[0], transitions[0]], [0, 35], 'b')
        plt.plot([transitions[1], transitions[1]], [0, 35], 'r')
        plt.plot([(transitions[2] + transitions[2]) / 2, (transitions[2] + transitions[2]) / 2], [0, 35], 'g')
        for j in range(5):
            reducenumber=10+j*5
            boundary=dic['boundariesterrible'][i][j]
            print(boundary[0][1])
            plt.scatter(boundary[0][0],reducenumber,marker='x',s=boundary[0][1]**2*1000+20)
    plt.show()
def prettypicture2():
    dict=pickle.load(open('Logs/prettypicture10','rb'))
    plt.figure()
    for i in range(7):
        pass
    plt.show()
def first_attempt_fix_valuelog():
    file=open('valuelog.txt','r')
    file2=open('valuelog2.txt','w')
    asdf=file.readline()
    kk=asdf.split('here')
    kkk=[i.split(')') for i in kk]
    print(kkk[1])
    for orders in kkk:
        for i in orders:
            file2.write(str(i))
            file2.write('\n')

def exactboundaries():
    tobesaved={}
    tobesaved['description']='tobesaved[i][j] contains a list of boundaries for sample i+3, with j+3 slow features'
    for i in range(7):
        sample=i+3
        data = generate_data.readSample(sample)
        transitions = generate_data.segmentations['86 0' + str(sample)]

        print('Working on sample '+str(sample))
        tobesaved[i]=[]
        for j in range(5):
            currentmiddleboundary = 0
            tobesaved[i].append([])
            reducenumber=10+j*5
            print('Using '+str(reducenumber)+' as reducenumber')
            while(currentmiddleboundary+800<transitions[-1]['upper']):
                for k in range(len(transitions)):
                    if currentmiddleboundary+200<transitions[k]['upper']: #We do the +300 thing so that we always have a reasonable transition.
                        if k+1<len(transitions):
                            endpoint=k+1
                        else:
                            endpoint=len(transitions)-1
                        break
                currentdata=data[currentmiddleboundary:transitions[endpoint]['upper']]
                print('Using window '+str(currentmiddleboundary)+' to '+str(transitions[endpoint]['upper']))
                currentdata=np.array([k for k in currentdata])
                boundary,binlist,featurevalues=GetClusteringFromData(currentdata,numfeatures=3,reducenumber=reducenumber)
                tobesaved[i][-1].append(boundary)
                boundaryvalue=boundary[0]
                if boundary[0]==0:
                    print('0 boundary detected. Artificially setting it to 50 to get things rolling')
                    boundaryvalue=50
                currentmiddleboundary=currentmiddleboundary+boundaryvalue
                print(currentmiddleboundary)
        os.remove('Logs/exactboundaries2')
        pickle.dump(tobesaved,open('Logs/exactboundaries2','wb'))
        print('*'*50)
        print('dumping')
        print('*'*50)
def exactboundariespicture():
    dic=pickle.load(open('Logs/exactboundaries2'))
    plt.figure()
    for i in range(len(dic.keys())-1):
        sample=i+3
        transitions = generate_data.segmentations['86 0' + str(sample)]
        plt.subplot(7,1,i+1)
        for j in range(len(dic[i])):
            numfeatures=10+j*5
            boundaries=dic[i][j]
            currentboundary=0
            actualboundaries=[]
            for boundary,error in boundaries:
                currentboundary+=boundary
                plt.scatter(currentboundary,numfeatures,marker='x',s=error**2*1000+20)
        for transition in transitions:
            plt.plot([transition['middle'],transition['middle']],[0,30])
    plt.show()

def windowsizeminibatch():
    tobesaved={}
    counter=logcounter('windowsizeminibatch')
    noise=0.01
    numblocks=15
    blocksize=500
    numfeatures=5
    tobesaved['data']=[]
    tobesaved['rightwindow']=[]
    tobesaved['smallwindow']=[]
    tobesaved['bigwindow']=[]
    tobesaved['terriblewindow']=[]
    for i in range(7):
        print('='*80)
        print(i)
        print('='*80)
        testdata,boundaries=generate_data.toydata2(noise=noise,numblocks=numblocks,blocksize=blocksize,numfeatures=numfeatures)
        tobesaved['data'].append((testdata,boundaries))
        print('Doing right window size')
        tobesaved['rightwindow'].append(minibatch(testdata,steplength=2*blocksize,numfeatures=numfeatures))
        print('Doing small window size')
        tobesaved['smallwindow'].append(minibatch(testdata,steplength= 2 * 0.75 * blocksize,numfeatures=numfeatures))
        print('Doing big window size')
        tobesaved['bigwindow'].append(minibatch(testdata, steplength=2 *1.25 * blocksize,numfeatures=numfeatures))
        print('Doing terrible window size')
        tobesaved['terriblewindow'].append(minibatch(testdata,steplength= 2 * 1.5* blocksize,numfeatures=numfeatures))
    pickle.dump(tobesaved,open('Logs/windowsizeminibatch'+str(counter),'wb'))

def windowsizeminibatch2():
    asdf=pickle.load(open('Logs/windowsizeminibatch2','rb'))
    plt.figure()
    plt.title('2=smallwindow, 4=right window, 6=slightly too big window, 8=way too big window')

    numsamples=len(asdf['rightwindow'])
    for i in range(numsamples):
        realboundaries=asdf['data'][i][1]
        plt.subplot(numsamples,1,i+1)
        plt.title('Random sample '+str(i+1))
        for boundary,error in asdf['smallwindow'][i]:
            print(error)
            plt.scatter(boundary,2,marker='x',s=(error**2*1000+20))
        for boundary,error in asdf['rightwindow'][i]:
            print(error)
            plt.scatter(boundary,4,marker='x',s=error**2*1000+20)
        for boundary,error in asdf['bigwindow'][i]:
            plt.scatter(boundary,6,marker='x',s=error**2*1000+20)
        for boundary,error in asdf['terriblewindow'][i]:
            plt.scatter(boundary,8,marker='x',s=error**2*1000+20)
        for boundary in realboundaries:
            plt.plot([boundary,boundary],[0,10])
    plt.show()

def highvariancedata(): #TODO rerun this. I am not willing to believe this is working correctly.
    counter=logcounter('highvariancedata')
    avgblocksize=500
    variance1=0.33
    variance2=0.66
    variance3=0
    tobesaveds=[]



    for i in range(6):
        print('In ' + str(i) + 'th iteration')
        tobesaved={}
        tobesaved['variance1'] = {}
        tobesaved['variance2'] = {}
        tobesaved['variance3'] = {}
        tobesaved['variance1']['data'] = []
        tobesaved['variance2']['data'] = []
        tobesaved['variance3']['data'] = []
        tobesaved['variance1']['boundaries'] = []
        tobesaved['variance2']['boundaries'] = []
        tobesaved['variance3']['boundaries'] = []
        tobesaved['variance1']['data'].append(generate_data.toydata2(numblocks=10, noise=0.001, variance=variance1, numfeatures=20, blocksize=avgblocksize))
        tobesaved['variance2']['data'].append(generate_data.toydata2(numblocks=10, noise=0.001, variance=variance2, numfeatures=20, blocksize=avgblocksize))
        tobesaved['variance3']['data'].append(generate_data.toydata2(numblocks=10, noise=0.001, variance=variance3, numfeatures=20, blocksize=avgblocksize))
        for j in range(7):
            scalefactor = 1 + (j - 3) * 0.06
            print('Using scalefactor:'+str(scalefactor))
            boundaries1=minibatch(tobesaved['variance1']['data'][-1][0],steplength=2*scalefactor*avgblocksize,numfeatures=5)
            print(boundaries1)
            tobesaved['variance1']['boundaries'].append(boundaries1)
            boundaries2=minibatch(tobesaved['variance2']['data'][-1][0],steplength=2*scalefactor*avgblocksize, numfeatures=5)
            tobesaved['variance2']['boundaries'].append(boundaries2)
            print(boundaries2)
            boundaries3=minibatch(tobesaved['variance3']['data'][-1][0], steplength=2*scalefactor*avgblocksize, numfeatures=5)
            tobesaved['variance3']['boundaries'].append(boundaries3)
            print(boundaries3)
        tobesaveds.append(tobesaved)
        pickle.dump(tobesaveds,open('Logs/highvariancedatavariablescalefactor','wb'))
def highvariancedata2():
    dic=pickle.load(open('Logs/highvariancedata2','rb'))
    numiterations=len(dic['variance1']['data'])
    plt.figure()
    for i in range(numiterations):
        plt.subplot(numiterations,3,i*3+1)
        trueboundaries=dic['variance1']['data'][i][1]
        print(dic['variance2']['boundaries'])
        for boundary,error in dic['variance1']['boundaries'][i]:
            plt.scatter(boundary,1,marker='x',s=error**2*1000+20)
        for boundary in trueboundaries:
            plt.plot([boundary,boundary],[0,2])
        plt.subplot(numiterations,3,i*3+2)
        trueboundaries=dic['variance2']['data'][i][1]
        for boundary,error in dic['variance3']['boundaries'][i]:
            plt.scatter(boundary,1,marker='x',s=error**2*1000+20)
        for boundary in trueboundaries:
            plt.plot([boundary,boundary],[0,2])
        plt.subplot(numiterations,3,i*3+3)
        trueboundaries=dic['variance3']['data'][i][1]
        for boundary,error in dic['variance3']['boundaries'][i]:
            plt.scatter(boundary,1,marker='x',s=error**2*1000+20)
        for boundary in trueboundaries:
            plt.plot([boundary,boundary],[0,2])
    plt.show()

def highvariancedatavariablesscalefactor():
    data=pickle.load(open('Logs/highvariancedatavariablescalefactor','rb'))
    plt.figure()
    for i in range(len(data)):
        dic=data[i]
        plt.subplot(len(data),3,3*i+1)
        realboundaries1=dic['variance1']['data'][0][1]
        for boundary in realboundaries1:
            plt.plot([boundary,boundary],[0.7,1.3])
        for j in range(len(dic['variance1']['boundaries'])):
            boundaries=dic['variance1']['boundaries'][j]
            scalefactor = 1 + (j - 3) * 0.06
            for boundary in boundaries:
                plt.scatter(boundary[0],scalefactor,marker='x',s=1000*boundary[1]**2+20)
        plt.subplot(len(data),3,3*i+2)
        realboundaries2 = dic['variance2']['data'][0][1]
        for boundary in realboundaries2:
            plt.plot([boundary,boundary],[0.7,1.3])
        for j in range(len(dic['variance2']['boundaries'])):
            boundaries=dic['variance1']['boundaries'][j]
            scalefactor = 1 + (j - 3) * 0.06
            for boundary in boundaries:
                plt.scatter(boundary[0],scalefactor,marker='x',s=1000*boundary[1]**2+20)
        plt.subplot(len(data),3,3*i+3)
        realboundaries3 = dic['variance3']['data'][0][1]
        for boundary in realboundaries3:
            plt.plot([boundary, boundary], [0.7, 1.3])
        for j in range(len(dic['variance1']['boundaries'])):
            boundaries=dic['variance1']['boundaries'][j]
            scalefactor = 1 + (j - 3) * 0.06
            for boundary in boundaries:
                plt.scatter(boundary[0],scalefactor,marker='x',s=1000*boundary[1]**2+20)
    plt.show()

def highvariancedata3():
    maxiterations=7
    scalefactors=[0.75,0.8,0.85,0.9,0.95,1,1.05,1.1,1.15,1.2,1.25]
    variances=[0,0.2,0.4]
    tobesaved=[]
    for i in range(maxiterations):
        variancelists=[]
        for j in range(len(variances)):
            dic={}
            data,boundaries=generate_data.toydata2(noise=0.001,variance=variances[j],numfeatures=30)
            dic['data']=data
            dic['realboundaries']=boundaries
            dic['variance']=variances[j]
            dic['scalefactors']=[]
            for scalefactor in scalefactors:
                print('Scalefactor: '+str(scalefactor))
                print('Variance: '+str(variances[j]))
                print('Iteration: '+str(i))
                dic['scalefactors'].append({})
                dic['scalefactors'][-1]['scalefactor']=scalefactor
                dic['scalefactors'][-1]['boundarydata']=minibatch(data,steplength=2*500*scalefactor,numfeatures=5,reducenumber=15,timesteps=20,numclusters=2)
            variancelists.append(dic)
        tobesaved.append(variancelists)
        pickle.dump(tobesaved,open('Logs/newhighvariancedata','wb'))

def highvariancedata3graph():
    plt.figure()
    tobesaved=pickle.load(open('logs/newhighvariancedata','rb'))
    for j in range(len(tobesaved)):
        variancelists=tobesaved[j]
        for i in range(len(variancelists)):
            plt.subplot(len(tobesaved),3,j*3+i+1)
            dic=variancelists[i]
            realboundaries=dic['realboundaries']
            for boundary in realboundaries:
                plt.plot([boundary,boundary],[0.65,1.3])
            for newdic in dic['scalefactors']:
                scalefactor=newdic['scalefactor']
                for boundary,error in newdic['boundarydata']:
                    plt.scatter(boundary,scalefactor,marker='x',s=1000*error**2+20)
    plt.show()

def doubletripletest():
    tobesaved=[[],[],[],[]]
    variances=[0,0.2,0.4,0.6]
    for i in range(4):
        variance=variances[i]
        for j in range(10):
            asdf, boundaries = generate_data.toydata2(variance=variance)

            transitions = generate_data.segmentations['86 01']
            kkk=tripledouble(asdf, numfeatures=5, steplength=500, reducenumber=15, timesteps=20)
            minibatcherino=minibatch(asdf,numfeatures=5,steplength=500*2,reducenumber=15,timesteps=20)
            print(boundaries)
            tobesaved[i].append((kkk,minibatcherino,boundaries,asdf))
            pickle.dump(tobesaved,open('Logs/doubletripletest','wb'))
def doubletriplediagram():
    asdf = pickle.load(open('Logs/doubletripletest', 'rb'))
    plt.figure()
    for i in range(len(asdf)):
        listerino=asdf[i]
        for j in range(len(listerino)):
            tdboundaries, mboundaries, rboundaries, data=listerino[j]
            plt.subplot(len(listerino),3,i+1+j*3)
            for boundary,status in tdboundaries:
                if status=='proper':
                    plt.scatter(boundary,1,marker='x',s=100)
                else:
                    plt.scatter(boundary,1,marker='x',s=500)
            for boundary,error in mboundaries:
                plt.scatter(boundary,2,marker='.')
            for boundary in rboundaries:
                plt.plot([boundary,boundary],[0,3])
    plt.show()
def realdatacomparison():

    tobesaved = [['Transition points found by minibatch'], ['Errors for those transitions']]
    counter = logcounter('prettypicturecounter')
    windowsize = [1200, 2000, 2000, 2000, 2000, 1800, 2000, 1800, 2000]
    numsamples = 7

    for j in range(numsamples):
        sizerino=windowsize[j]
        sample = j + 3
        # plt.subplot(numsamples,1,j+1)
        transitions = generate_data.segmentations['86 0' + str(sample)]
        print('Looking at sample' + str(sample) + '\n')
        data = generate_data.readSample(sample)
        testdata = np.array([k for k in data])
        print('='*50)
        print('minibatch right now')
        boundarymb=minibatchfromsample(sample,steplength=sizerino,reducenumber=15,numfeatures=5,timesteps=20)
        print('='*50)
        print('doubletriple right now')
        boundarydt=tripledouble(testdata,numfeatures=5,steplength=sizerino/2,reducenumber=15,timesteps=20)
        tobesaved.append((boundarymb,boundarydt))

        pickle.dump(tobesaved, open('Logs/doubletriplecomparisionrealdata', 'wb'))
def realdatacomparisondiagram():
    data=pickle.load(open('Logs/doubletriplecomparisionrealdata','rb'))
    realdata=data[2:]
    plt.figure()
    for j in range(len(realdata)):
        sample=j+3
        transitions = generate_data.segmentations['86 0' + str(sample)]
        plt.subplot(len(realdata),1,j+1)
        boundarymb,boundarydt=realdata[j]

        for point in boundarymb[0]:
            plt.scatter(point,2,marker='.')
        for point,status in boundarydt:
            if status == 'proper':
                plt.scatter(point, 1, marker='x', s=100)
            else:
                plt.scatter(point, 1, marker='x', s=500)
        for x in transitions:
            plt.plot([x['middle'],x['middle']],[0,3])
    plt.show()

def smoldatatest():
    windowsize = [1200, 2000, 2000, 2000, 2000, 1800, 2000, 1800, 2000]
    length=len(generate_data.othersegmentationstuff)

    # for i in range(length):
    #     plt.subplot(length,1,i+1)
    #     sample=i+1
    #     data=generate_data.smoldata(sample)
    #     haca=generate_data.othersegmentationstuff[i]
    #     for point in haca:
    #         plt.scatter(point,1,marker='x')
    #     mb=minibatch(data,windowsize[i]/4)
    #     for point,error in mb:
    #         plt.scatter(point,2,marker='x')
    for j in range(9):
        plt.figure()
        sample=1+j
        reducenumber=20
        groundtruth = generate_data.segmentations['86 0'+str(sample)]
        numclusters=len(groundtruth)
        num_clusters=[numclusters-2,numclusters-1,numclusters,numclusters+1,numclusters+2,numclusters+3,numclusters+3]
        print('Sample: '+str(sample))
        print('smol')
        data = np.array(generate_data.smoldata(sample))
        testdata = pca_reduce(data, reducenumber)
        haca = generate_data.othersegmentationstuff[sample - 1]
        sfanode = mdp.nodes.SFANode()
        expansionnode = mdp.nodes.PolynomialExpansionNode(2)
        testdata2 = expansionnode.execute(testdata)
        print(testdata2.shape)
        sfanode.train(testdata2)
        sfanode.stop_training()
        feature_values = sfanode.execute(testdata2, n=5)
        distmatrix = generate_data.simmatrix2(5, feature_values, 20)
        for i,clusternum in enumerate(num_clusters):
            plt.subplot(len(num_clusters),2,2*i+1)
            clusterer = cluster.SpectralClustering(n_clusters=clusternum, affinity='precomputed')
            binarylist = clusterer.fit_predict(distmatrix)
            binarylist = generate_data.reorder(binarylist)
            plt.scatter(range(len(binarylist)),binarylist,marker='x')

            groundtruth2=[point['middle']/4. for point in groundtruth]
            for point in groundtruth2:
                plt.plot([point,point],[0,10])

        print('big')
        data = np.array(generate_data.readSample(sample))
        testdata = pca_reduce(data, reducenumber)
        haca = generate_data.othersegmentationstuff[sample - 1]
        sfanode = mdp.nodes.SFANode()
        expansionnode = mdp.nodes.PolynomialExpansionNode(2)
        testdata2 = expansionnode.execute(testdata)
        print(testdata2.shape)
        sfanode.train(testdata2)
        sfanode.stop_training()
        feature_values = sfanode.execute(testdata2, n=5)
        distmatrix = generate_data.simmatrix2(5, feature_values, 20)
        for i, clusternum in enumerate(num_clusters):
            plt.subplot(len(num_clusters), 2, 2*i + 2)
            clusterer = cluster.SpectralClustering(n_clusters=clusternum, affinity='precomputed')
            binarylist = clusterer.fit_predict(distmatrix)
            binarylist = generate_data.reorder(binarylist)
            plt.scatter(range(len(binarylist)), binarylist, marker='x')

            groundtruth2 = [point['middle'] for point in groundtruth]
            for point in groundtruth2:
                plt.plot([point, point], [0, 10])
    plt.show()
def smoltest2():
    plt.figure()
    sample=8
    reducenumber = 20
    groundtruth = generate_data.segmentations['86 0' + str(sample)]
    numclusters = len(groundtruth)
    num_clusters = [numclusters - 2, numclusters - 1, numclusters, numclusters + 1, numclusters + 2, numclusters + 3, numclusters + 3]
    print('Sample: ' + str(sample))
    print('smol')
    data = np.array(generate_data.smoldata(sample))
    testdata = pca_reduce(data, reducenumber)
    haca = generate_data.othersegmentationstuff[sample - 1]
    sfanode = mdp.nodes.SFANode()
    expansionnode = mdp.nodes.PolynomialExpansionNode(2)
    testdata2 = expansionnode.execute(testdata)
    print(testdata2.shape)
    sfanode.train(testdata2)
    sfanode.stop_training()
    feature_values = sfanode.execute(testdata2, n=5)
    distmatrix = generate_data.simmatrix2(5, feature_values, 20)
    expected_windowsize=1000/4 #specific value for sample 8
    for i, clusternum in enumerate(num_clusters):
        plt.subplot(len(num_clusters), 1, 1 * i + 1)
        clusterer = cluster.SpectralClustering(n_clusters=clusternum, affinity='precomputed')
        binarylist = clusterer.fit_predict(distmatrix)
        binarylist = generate_data.reorder(binarylist)
        binarylist=generate_data.reorder(binarylist)
        binarylist=generate_data.clusteringheuristic1(binarylist,expected_windowsize)
        binarylist=generate_data.clusteringheuristic2(binarylist,expected_windowsize,distmatrix)
        boundary1=generate_data.findbestcenterapproach(binarylist,clusternum,expected_windowsize)
        boundary2=generate_data.find_boundarieskmeans(binarylist,clusternum,expected_windowsize/2)

        plt.scatter(range(len(binarylist)), binarylist, marker='x')
        for boundary in boundary1:
            plt.plot([boundary[0],boundary[0]],[11,12],'red')
        for boundary in boundary2:
            plt.plot([boundary,boundary],[13,14],'blue')
        groundtruth2 = [point['middle']/4. for point in groundtruth]
        for point in groundtruth2:
            plt.plot([point, point], [0, 5])
        for point in haca:
            plt.plot([point,point],[5,10])
    plt.show()
def smoldtwtest():
    sample=8
    reducenumber=20
    expected_windowsize = 1000 / 3 # specific value for sample 8
    groundtruth = generate_data.segmentations['86 0' + str(sample)]
    numclusters = len(groundtruth)
    data = np.array(generate_data.smoldata(sample))
    testdata = pca_reduce(data, reducenumber)
    haca = generate_data.othersegmentationstuff[sample - 1]
    sfanode = mdp.nodes.SFANode()
    expansionnode = mdp.nodes.PolynomialExpansionNode(2)
    testdata2 = expansionnode.execute(testdata)
    print(testdata2.shape)
    sfanode.train(testdata2)
    sfanode.stop_training()
    feature_values = sfanode.execute(testdata2, n=5)
    matrix1,matrix2,matrix3=pickle.load(open('logs/dtwstuff','rb'))
    #dumperino=[]
    #distmatrix1=generate_data.simmatrix2(5,feature_values,20)
    #dumperino.append(distmatrix1)
    #pickle.dump(dumperino,open('Logs/dtwstuff','wb'))
    newmatrix2=generate_data.hybriddtwmatrix(5,feature_values,20,expected_windowsize)
    #dumperino.append(distmatrix3)
    #pickle.dump(dumperino,open('logs/dtwstuff','wb'))

    #distmatrix2=generate_data.slowestdtwmatrix(5,feature_values,20)
    dumperino=[matrix1,newmatrix2,matrix3]
    pickle.dump(dumperino,open('Logs/dtwstuff','wb'))
    #plt.matshow(distmatrix1)
    #plt.matshow(distmatrix2)
    #plt.matshow(distmatrix3)
    #plt.show()
def makeclustersfromboundaries(binarylist,boundaries):
    clusters=[[] for boundary in boundaries]+[[]]
    for i,clustering in enumerate(binarylist):
        donezo=True
        for j,point in enumerate(boundaries):
            if i<point:
                clusters[j].append((i,clustering))
                donezo=False
                break
        if donezo:
            clusters[-1].append((i,clustering))
    return clusters


    pass
def clusterevaluation1(binarylist,clusters):
    '''
    Evaluates a clustering created by makeclustersfromboundaries, and a standard binarylist as everywhere else in these files. Evaluates it by seeing how homogenous it is, aka what ratio the most common has to everything else.
    Returns that ratio
    '''
    scores=[]
    for cluster in clusters:
        if cluster!=[]:
            clusteringlist=[x[1] for x in cluster]
            mostcommon=max(set(clusteringlist), key=clusteringlist.count)
            scores.append(clusteringlist.count(mostcommon)/float(len(clusteringlist)))
        else:
            scores.append(0)
    return scores

def clusterevaluation2(binarylist,clusters,distmatrix):
    '''
    Evaluates a clustering created by makeclustersfromboundaries, and a standard binarylist as everywhere else in these files, and the corresponding distance matrix. Computes the sum of the similarity scores of the clustering (box on distmatrix), then divides by n^2 (basically average similarity)
    '''
    scores=[]
    for cluster in clusters:
        if cluster!=[]:
            sum=0
            for point1 in cluster:
                coordinate1=point1[0]
                for point2 in cluster:
                    coordinate2=point2[0]
                    sum+=distmatrix.item(coordinate1,coordinate2)
            sum/=len(cluster)**2
            scores.append(sum)
        else:
            scores.append(0)
    return scores
def smoltest3():
    plt.figure()
    sample=8
    reducenumber = 20
    groundtruth = generate_data.segmentations['86 0' + str(sample)]
    numclusters = len(groundtruth)
    num_clusters = [numclusters - 2, numclusters - 1, numclusters, numclusters + 1, numclusters + 2, numclusters + 3, numclusters + 3]
    print('Sample: ' + str(sample))
    print('smol')
    data = np.array(generate_data.smoldata(sample))
    testdata = pca_reduce(data, reducenumber)
    haca = generate_data.othersegmentationstuff[sample - 1]
    sfanode = mdp.nodes.SFANode()
    expansionnode = mdp.nodes.PolynomialExpansionNode(2)
    testdata2 = expansionnode.execute(testdata)
    print(testdata2.shape)
    sfanode.train(testdata2)
    sfanode.stop_training()
    feature_values = sfanode.execute(testdata2, n=5)
    distmatrix = generate_data.simmatrix2(5, feature_values, 20)
    expected_windowsize=1000/4 #specific value for sample 8
    for i, clusternum in enumerate(num_clusters):
        plt.subplot(len(num_clusters), 1, 1 * i + 1)
        clusterer = cluster.SpectralClustering(n_clusters=clusternum, affinity='precomputed')
        binarylist = clusterer.fit_predict(distmatrix)
        binarylist = generate_data.reorder(binarylist)
        binarylist=generate_data.reorder(binarylist)
        binarylist=generate_data.clusteringheuristic1(binarylist,expected_windowsize)
        binarylist=generate_data.clusteringheuristic2(binarylist,expected_windowsize,distmatrix)
        boundary1=generate_data.findbestcenterapproach(binarylist,clusternum,expected_windowsize)
        boundary2=generate_data.find_boundarieskmeans(binarylist,clusternum,expected_windowsize/2)
        boundary12=[x[0] for x in boundary1[1:]]
        clusters1=makeclustersfromboundaries(binarylist,boundary12)
        clusters2=makeclustersfromboundaries(binarylist,boundary2)
        evaloneone=clusterevaluation1(binarylist,clusters1)
        evaltwoone=clusterevaluation1(binarylist,clusters2)
        evalonetwo=clusterevaluation2(binarylist,clusters1,distmatrix)
        evaltwotwo=clusterevaluation2(binarylist,clusters2,distmatrix)
        plt.scatter(range(len(binarylist)), binarylist, marker='x')

        for i,boundary in enumerate(boundary1):
            plt.scatter(boundary[0],[11],marker='.',c='red',s=(1-evaloneone[i])**2*1000+30)
            plt.scatter(boundary[0],[15],marker='.',c='blue',s=(1-evalonetwo[i])**2*1000+30)
        for i,boundary in enumerate(boundary2):
            plt.scatter(boundary,[19],marker='x',c='red',s=(1-evaltwoone[i])**2*1000+30)
            plt.scatter(boundary,[23],marker='x',c='blue',s=(1-evaltwotwo[i])**2*1000+30)
        groundtruth2 = [point['middle']/4. for point in groundtruth]
        for point in groundtruth2:
            plt.plot([point, point], [0, 5])
        for point in haca:
            plt.plot([point,point],[5,10])
    plt.show()
def generatematrices():
    euclidmatrix,hybridmatrix,fulldtwmatrix=pickle.load(open('logs/dtwstuff','rb'))
    euclidwith0s=[]
    for i,row in enumerate(euclidmatrix):
        euclidwith0s.append([])
        for j,entry in enumerate(row.tolist()[0]):
            if abs(i-j)<400:
                euclidwith0s[-1].append(euclidmatrix.item(i,j))
            else:
                euclidwith0s[-1].append(0)
    euclidwith0s=np.matrix(euclidwith0s)
    dtwwith0s=[]
    for i,row in enumerate(euclidmatrix):
        dtwwith0s.append([])
        for j,entry in enumerate(row.tolist()[0]):
            if abs(i-j)<400:
                dtwwith0s[-1].append(fulldtwmatrix.item(i,j))
            else:
                dtwwith0s[-1].append(0)
    dtwwith0s=np.matrix(dtwwith0s)
    return[euclidmatrix,fulldtwmatrix,hybridmatrix,euclidwith0s,dtwwith0s]

def smoltest4():
    plt.figure()
    reducenumber = 20
    groundtruth = generate_data.segmentations['86 0' + str(8)]
    numclusters = len(groundtruth)
    haca = generate_data.othersegmentationstuff[8 - 1]
    sfanode = mdp.nodes.SFANode()
    expected_windowsize = 1000 / 4  # specific value for sample 8
    euclidmatrix,fulldtwmatrix,hybridmatrix,euclidwith0s,dtwwith0s=generatematrices()
    labels=['Full Euclidean','Full Dtw','Hybrid Euclidean and DTW','Euclidean with 0s','Dtw with 0s']
    matrices=[euclidmatrix,fulldtwmatrix,hybridmatrix,euclidwith0s,dtwwith0s]
    for i,distmatrix in enumerate(matrices):
        plt.subplot(len(matrices), 1, 1 * i + 1)
        clusterer = cluster.SpectralClustering(n_clusters=numclusters, affinity='precomputed')
        binarylist = clusterer.fit_predict(distmatrix)
        binarylist = generate_data.reorder(binarylist)
        binarylist=generate_data.reorder(binarylist)
        binarylist=generate_data.clusteringheuristic1(binarylist,expected_windowsize)
        binarylist=generate_data.clusteringheuristic2(binarylist,expected_windowsize,distmatrix)
        boundary1=generate_data.findbestcenterapproach(binarylist,numclusters,expected_windowsize)
        boundary2=generate_data.find_boundarieskmeans(binarylist,numclusters,expected_windowsize/2)
        boundary12=[x[0] for x in boundary1[1:]]
        clusters1=makeclustersfromboundaries(binarylist,boundary12)
        clusters2=makeclustersfromboundaries(binarylist,boundary2)
        evaloneone=clusterevaluation1(binarylist,clusters1)
        evaltwoone=clusterevaluation1(binarylist,clusters2)
        evalonetwo=clusterevaluation2(binarylist,clusters1,distmatrix)
        evaltwotwo=clusterevaluation2(binarylist,clusters2,distmatrix)
        plt.title(labels[i])
        plt.scatter(range(len(binarylist)), binarylist, marker='x')

        for i,boundary in enumerate(boundary1):
            plt.scatter(boundary[0],[11],marker='.',c='red',s=(1-evaloneone[i])**2*1000+30)
            plt.scatter(boundary[0],[15],marker='.',c='blue',s=(1-evalonetwo[i])**2*1000+30)
        for i,boundary in enumerate(boundary2):
            plt.scatter(boundary,[19],marker='x',c='red',s=(1-evaltwoone[i])**2*1000+30)
            plt.scatter(boundary,[23],marker='x',c='blue',s=(1-evaltwotwo[i])**2*1000+30)
        groundtruth2 = [point['middle']/4. for point in groundtruth]
        for point in groundtruth2:
            plt.plot([point, point], [0, 5])
        for point in haca:
            plt.plot([point,point],[5,10])
    plt.show()
#first_attempt()
#second_attempt()
#minibatchtest()
#minibatchprettypicture()
#windowsizetest()
#manywindows()
#numfeaturesminibatch()
#windowsize2()
#exactboundaries()
#exactboundariespicture()
#windowsizeminibatch()
#windowsizeminibatch2()
#highvariancedata()
#highvariancedatavariablesscalefactor()
#highvariancedata3graph()
#doubletripletest()
#doubletriplediagram()
#realdatacomparison()
#realdatacomparisondiagram()
#smoldatatest()
#smoltest2()
#smoldtwtest()
#smoltest3()
