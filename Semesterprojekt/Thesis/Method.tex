\section{Basic Method}
\ednote{We need to define at some point what exactly we're looking for in terms of boundary points. Define input data, and then that we need cutting points}


Paragraph about why SFA is good here.

Paragraph about why we want to use selfsimilarity. (Motions are made of many oft repeated motions)

Thus our goal is to investigate an algorithm that segments motion data using SFA and using the principle of self similarity to decide where to place good boundary points segmenting the data.

To do this we first apply SFA to the data, then construct a fitting similarity matrix from the gained slow features, apply spectral clustering to this similarity matrix, and finally find boundary points based on this clustering.
\ednote{Is this enough of a transition?}
\ednote{Somewhere we need to get the dimensionality reduction as preprocessing in.}
\subsection{Slow Feature Analysis}
\ednote{Find Paper to cite for slow feature analysis: Probably using this one: http://ieeexplore.ieee.org/document/6790128/}
We base our description on SFA on \ednote{add citation here}.

SFA is based on the principle that meaningful, global change in data happens slowly, compared to the possibly quick oscillations that local sensors can identify. If we were to look at a video of a zebra running in front of the camera, local sensors would quickly detect large changes in the color, due to the zebras stripes, however the global movement of the zebra would be much slower.

\ednote{Add part about conditions for problem that SFA solves}

Suppose we have a $k$-dimensional input signal $x(t)$.

The first step of SFA is to normalize the data $x(t)$ to $\tilde{x}(t)$, where $\tilde{x}(t)$ has $0$ mean and unit variance.

Then we expand $\tilde{x}(t)$ with some nonlinear functions $h(x)$. A standard choice for this are all monomials of degree $1$ and $2$. This is done so that SFA can also recognize nonlinear relationships in the data.

As the last step of preprocessing, we whiten the expanded $z(t)=h(\tilde{x}(t))$ to receive $\tilde{z}(t)$, with again $0$ mean and with unit covariance matrix $\tilde{z}\tilde{z}^T=I$. \ednote{Possibly describe how whitening is done, it's just PCA again}

As an intermediary step, we make the matrix $Z=\dot{\tilde{z}}\dot{\tilde{z}}^T$, which is just the covariance matrix of the temporal difference vectors $\dot{\tilde{z}}(t)=\tilde{z}(t+1)-\tilde{z}(t)$.

Finally we apply PCA \ednote{check whether this is first occurrence of PCA, do we need to define what PCA is? } to $Z$ to get a list of eigenvectors and eigenvalues $v_i$ and $\lambda_i$ respectively. We use these to define our slow features $g_i$, with $g_i=v_i^T\tilde{z}(t)$ and $\lambda_1 \geq \lambda _2 \leq \ldots \geq \lambda_k$. 

We note that we use the smallest eigenvalues instead of the largest eigenvalues, since we want the slowest changing features.

We can then use these feature values to construct a similarity matrix, that we can later use for clustering.
\subsection{Make Similarity Matrix}
\ednote{Think about possibly explaining closer what a similarity matrix is. But it's not really a well defined formal concept}
The first step in making a similarity matrix is turning our sequence of feature values $g_1(t),g_2(t),\ldots,g_k(t)$  into a sequence of feature vectors. These feature vectors will then be compared to each other and their similarity scores will make up our similarity matrix.

Our first choice is thus how are feature vectors are composed \ednote{This sounds dumb}, most notably how many features we use and how many timesteps we are comparing at once.

The choice of how many features to consider obviously depends on the data that we use, if we expect there to be many relevant features, maybe because there are many semantically relevant slow changing parts of the sequence, e.g. there are many people running in a video, we should be using a high number of features in our feature vectors. If the data is relatively simple or we have a limited amount of computing power, then a lower number might be more appropriate. \ednote{This entire paragraph sounds dumb. Look at it later} For our experiments, we defaulted to using 5 features. \ednote{Is using we for authors proper in non math papers? }

After choosing the number of features to consider, we must consider how many time steps to incorporate in our feature vectors. This will again be affected \ednote{affect or effect? I think it's affect} by our data, if we expect the segments to be about $100$ points long, then using $50$ time steps will cover a much larger portion of a segment than if we expected segments to be $1000$ points long. \ednote{Is it good to be specific here?} Four our experiments, we defaulted to using time steps of \ednote{time steps or timesteps?} 20, which seemed to work well regardless of data structure.

Once we have chosen the number $n_{feature}$ of features and the number $n_{time}$ of time steps to consider, we construct our feature vectors $v(t)$ using:
\[
v(t)= \left (
\begin{matrix}
g_1(t) \\
g_2(t) \\
\vdots \\
g_{n_{feature}}(t) \\
g_{1}(t+1) \\
\vdots \\
g_{n_{feature}}(t+n_{time})
\end{matrix}
\right ) = \left (
\begin{matrix}
G_{n_{feature}}(t) \\
G_{n_{feature}}(t+1) \\
\vdots \\
G_{n_{feature}}(t+n_{time})
\end{matrix}
\right )
\]
\ednote{lko:Is the $G$ stuff understandable? Or do I specifically need to define it? }
The simplest way to make a similarity matrix is to simply take the scalar product as a similarity measure. However this has the issue that the resulting scores are not normalized, and that it is not independent of the scaling of vectors. If we take two vectors $v$ and $\lambda v$ with $\lambda \leq 1$, then $v*v \geq \lambda v* \lambda v=\lambda^2 v*v$, even though both are identical. 

To remedy this, we use $e^{-d(v,v^*)/\Delta}$ as our similarity measure, for some vectors $v,v*$, some distance measure $d$, and some scalefactor $\Delta > 0$. This has the property that we only get similarities from $0-1$, and that we get a score of $1$ iff \ednote{lko:Do we write this out? or leave it as iff} the two vectors are identical. The issue with different scaling described for the previous points might still apply, however we are more interested in similar points, so this should be a minor issue \ednote{Is this understandable}.

The choice of $\Delta$ depends on the given data and previous parameters chosen. If we choose a very small $\Delta$, then most of the similarities will be forced to go to $0$, but if we make it too large we will get many results that are too close to $1$. A good choice of $\Delta$ ensures that the the interval $[0,1]$ is filled relatively evenly. 
\ednote{Put in picture of matrix done with basic euclidean matrix}

Thus we are left with a similarity matrix $S$ with $(S)_{i,j}=e^{-d(v(i),v(j)/\Delta}$, where the last relevant choice is the choice of distance measure $d$.
\subsubsection{DTW versus Euclidean}
The main distances that we used were \ednote{look for dtw paper} using the simple euclidean distance and using the Digital Time Warping (DTW) distance. 

From a purely theoretical perspective, DTW would seem more appropriate, since it takes into account possible misalignment of two sequences. However it has the heavy drawback that computing $d_{dtw}$ the DTW distance between two sequences is $O((n_{features}*n_{time})^2)$, while euclidean distance can be computed in $O(n_{features}*n_{time})$ \ednote{check whether its big o or small o}. In practice it was not uncommon for the larger matrices to take about a minute to compute with using Euclidean distance and taking over $10$ hours to compute using DTW.\ednote{dtw stuff here. Do I have to define dtw? }

Thus when considering using DTW, we use several tricks to reduce the time needed for computation.

We first use some insight about the structure of our feature vectors $v(t)$. Notably we use that our vectors $v(t)$ consist of subvectors $G(t)=(g_1(t) \ldots g_{num_{feature}}(t))$. Since $g_i$ are separate orthogonal features, we expect that the transition from $g_{num_{feature}}(t)$ to $g_1(t+1)$ would have no similarities. \ednote{Ehh this doesn't really work. I want something like any DTW path would go diagonally here} Thus it is theoretically sound to use $d_{dtw}(v_(t),v^*(t0)=\sum\limits_{i=0}^{num_{feature}} d_{dtw} (G_i(t),G^*_i (t))$, i.e. we are only aligning the same features to each other. This reduces the complexity to $O(num_{feature}num_{time}^2)$, which is a significant gain. 

The final trick that we use is that we are mostly interested in local similarities, as we have to find boundary points that separate neighboring clusters and thus it is not as important how similar two feature vectors at the end and the start are, it is much more important to know how similar close points are. Thus the idea is that for close vectors, we use the superior DTW distance, and for far away vectors, we use euclidean distance, or even set them to $0$ altogether. 

\ednote{lko: I have to rethink whether I want to use standard euclidean distance, or piecewise taxicab. I know what's meant}
A useful property of DTW distance that we use here is that $d_{dtw}(x,y)\leq d_{euclidean}(x,y) \forall x,y$, since  the DTW path would at worst be just the diagonal \ednote{lko: not happy about this but have raid now.}. Thus $e^{-d_{dtw}(x,y)}\geq e^{d_{euclidean}(x,y)}$, i.e. we possibly make distant points less similar than they should be. This is potentially an upside, as it reduces the chance of clustering techniques giving distant points the same clustering, which can make finding accurate boundary points difficult.

However the first step to being able to find a boundary is clustering in the first place. We do this using Spectral Clustering

\ednote{lko: Put picture of matrix here. Still need to make it though. Near the diagonal dtw, far away from the diagonal euclidean}
\ednote{We probably want pictures of matrices here. Not sure if based on toydata or based on actual data} 
\subsection{Spectral Clustering}
\ednote{Motivation for spectral clustering here. Not sure about it tbh, it's just the standard approach to use.}
We choose to use spectral clustering because it is simple to implement, is based on standard linear algebra, similar to SFA, and because initial testing with other clustering techniques such as k-means clustering did not lead to good results. \ednote{lko: pretty ehhhh on this sentence tbh} We follow \ednote{citation here I guess} in their description of spectral clustering.

There are several different variants of this technique, varying in the choice of similarity matrix and whether they normalize eigenvectors/values or not. However at their heart, they all follow the same sequence of steps.

Given a similarity matrix $S \in \mathbb{R}^{T  T}$ and a number $k$ of desired clusters, we first start by computing the unnormalized Laplacian $L=D-S$, where $D$ is the diagonal of $S$.

Next we compute the $k$ eigenvectors corresponding to the smallest $k$ eigenvalues \ednote{lko: Do we repeat $k$ here? Not sure if necessary or stylitically nice} $u_1,\ldots,u_k$ of $L$, which we use to construct the matrix $U\in \mathbb{R}^{T \times k}$, which has the eigenvectors $u_i$ as columns.

We define $y_i$ to be the vector corresponding to the $i$-th row of $U$. This corresponds to taking the $i$-th elements of the first $k$ eigenvectors \ednote{lko: In principle this is not well defined, but we explained which vectors we were taking earlier, so it should be fine}.

Then we use k-means clustering to cluster these \ednote{lko: Do we need to explain k-means clustering? I might want to do it later. Not sure at all. It's a very basic clustering technique.} $y_i$, into clusters $C_j$.

We are left with clusters $C_1,\ldots, C_k$ that assign each feature vector $v(t)$ an integer representative of their cluster. However, this clustering could be relatively arbitrary, it is common that we get assign e.g. time $1$ to cluster 1, time $2$ to cluster $2$, but time $3$ to cluster $1$ again. We are looking to segment our time series data, i.e. we need to find points that act as boundaries for a clustering, while minimizing errors.
\subsection{Find decision boundaries}
As always when minimizing an error, we first have to define an error function. 

To do this we first have to decide what error we want to minimize. There are two potential ways to approach this, we can try to minimize error in the similarity matrix or minimize error in the clustering. \ednote{lko: This whole sequence is bad imo}

If we were to minimize errors in the similarity matrix, we would need to find boundary points that result in the smallest and least amount of misclassifications, if we consider every point as represented by their vector in the similarity matrix, or even as represented by just the values of their slow features.

The other option would just require that we represent every point by the cluster-number assigned by spectral clustering, and try to minimize the number of misclassifications \ednote{lko: is misclassifications a word?} that would result in a specific boundary. 

We go with the second option, since even finding a good error function for the first would be tricky, and would come too close to making the effort of spectral clustering useless \ednote{Okay this entire sequence is fucky, and needs to be reworked or removed. Might be a discussion worth having, so I'm writing something on it, but I'm just leaving it in because removing is easier than adding} 

Thus we define our error function for the second interpretation. We consider boundary points $b_1,\ldots,b_k$ with the accompanying function $g(t,b_1,\ldots,b_k)$ assigning the point in time $t$ the value $j$ if $j$ is the most common clustering in the interval $[b_t,b_{t+1})$ and the function $c(t)=j$ if $y_t \in C_j$. For convenience sake we assume that the center of $C_i$ is lower than the center of $C_j$ iff \ednote{lko: again check iff} $i<j$. Then we get the error function \ednote{check if $T$ is the appropriate delimitor}
\[
E(b_1,\ldots,b_k)=\sum_{i=1}^T \delta_{g(t),c(t)}
\]
where $\delta_{i,j}$ is the Kronecker delta \ednote{check if spelling is correct}. Thus for a set of boundary points, we are simply checking how many points would be misclassified if we compare the clustering we get from spectral clustering to the clustering imposed by these boundary points.

This leads us to the optimization problem of finding the minimum $b^*_1,\ldots,b^*_k$ with 
\[
b_1^*,\ldots,b_k^*=\argmin_{b_1,\ldots,b_k} E(b_1,\ldots,b_k) 
\]
We can immediately see that if we only need one or two boundary points, it's viable to just try out all options, however the cost increases exponentially with the amount of boundary points.


How many boundary points are needed is one of the key problems that we work around in the various specific implementations of the entire algorithm that we use. \ednote{lko: Dis transition is bad and I should feel bad. But transitions are what I'll need to fix when I have everything written, so YOLO}
\ednote{Maybe put a summary in here somewhere?}
\ednote{Potentially put reducenumber stuf in here}
\ednote{Transition}