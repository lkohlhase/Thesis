\section{Specific Details}
In the previous section, we discussed the core steps of our algorithm, but we left out specific implementations. \ednote{lko:I really do not have this transition stuff down atm}

\ednote{The flow here should be: First batch, then minibatch as answer to the question: Into how many things do you want to cluster? Then fully online as answer to the question of how big do you want windows to get.}
In our testing, three different variants of the basic algorithm crystallized, which essentially differed in how online the approach was.

We will first describe the fully offline approach, then the intermediate approach, and finally the fully online approach.
\subsection{Batch}
The basic idea behind the fully offline approach is very simple, given some sequence of input data $x(1), \ldots, x(T)$ with $x(t) \in \mathbb{R}^n$, we first generate $k$ slow features $g_i(t)$ using SFA on the entirety of our data, then we make feature vectors $v(t)$ as described previously \ednote{this should be enough right?}, use spectral clustering to cluster the data into $k_{cluster}$ different clusters, and then find the decision boundaries.

This short description leaves two major questions open: How do we find decision boundaries and How many different clusters are we looking for?

We will first look into how we could answer the second question. To answer it, we have to define what we regard as a good result of finding boundaries, especially if we use a variable number of boundaries. Intuitively, the most important thing is finding the correct boundaries potentially finding some extraneous \ednote{potentially unnecessary} is not that big of a deal \ednote{very colloquial}. Thus when looking at test data to answer the second question, we would look at the best decision boundaries, and see how close they are to the real decision boundaries.

Thus, to have a chance at answering the second question, we must be able to find decision boundaries in the first place. Hence we have to first answer the first question.
\ednote{We need to invent some way of formalizing this intuition}

Given a list $c_1,\ldots,c_{T}$ of length $T$ \ednote{actually T-$n_{time}$} of integers from $1$ to $k_{cluster}$, and a list of true boundary points $b^*_1,\ldots, b^*_{k_{true}}$ Our goal is to use only $c_1,\ldots, c_{T}$, to find $k_{cluster}$ boundary points $b_1,\ldots,b_{k_{cluster}}$ with $b_1 < b_2 < \ldots <b_{k_{cluster}}$ so that $b_i$ are close to $b^*_j$. We consider the best $b_i$ for each $b^*{j}$, even if $b_{true}>b_{cluster}$. \ednote{lko:This isn't super formal either REEEEEEEEEE}

Clearly without access to $b^*$, our best bet is to minimize some error function using only $c_i$. We had previously defined an error function 
\[
E(b_1,\ldots,b_k)=\sum_{i=1}^T \delta_{g(t),c_t}
\]
As a reminder, we defined by $g(t)$ the clustering induced by our boundary points, where $g(t)$ is equal to the most common element in the interval $[b_{i},b_{i+1})$, for $t \in [b_i,b_{i+1})$ \ednote{lko: Is this phrased nicely? Should be just that it's actually in that interval}. Then finding the optimal boundary points is just finding the answer to
\[
b_1,\ldots,b_{k_{cluster}}=\argmin_{b_1,\ldots,b_{k_{cluster}}} E(b_1,\ldots,b_k) 
\]
\ednote{check if the prime shows well}
\ednote{I forgot, do you do a hanging . here?}
The naive solution to this is just trying every combination of points, and then evaluating $E$. However while this will certainly find the optimal $b_1,\ldots,b_k$, however it grows exponentially in $k_{cluster}$ and is thus only viable for finding one or two boundary points at most.

Thus for more complex problems, we must try to find a good approximation. \ednote{Probably want to add a figure of what clusterings can look like. Something like 2 by 2 with different data. Make sure to include messy shit}
\subsubsection{Finding Approximate Boundaries}
At first glance, finding good boundaries is trivial, since most clustering algorithms such as k-means clustering or \ednote{support vector machine. Not really a method of clustering, but a way of finding a boundary} implicitly define decision boundaries, which we would just need to make explicit. However these decision boundaries would lie \ednote{is this the spelling?} in the two-dimensional space of $(time,clustering)$. The boundary points that we are looking for are one dimensional points in time, thus some care has to be taken. 

\ednote{We formalized what the list would like above. Check that everything is consistent}

One primitive approach, was to find for every clustering $i \in [1,\ldots,n_{cluster}]$ the window of size $w_{s}$ \ednote{terrible variable name. Look for better one later} that contains the maximum amount of labels \ednote{check if we called them labels earlier. I really need to get this stuff consistent} $c_i$ corresponding to this clustering. Then we simply use the start of this window as a boundary point. 
\ednote{pseudo code here. Make sure it's nice.}
boundaries=[]
For clustering in $[1,\ldots,n_{cluster}]$:
clusteringcount=0
bestpoint=0
for point in $[1,\ldots,len(list)-windowsize]$:
if [point,point+windowsize].count(clustering)>clusteringcount:
clusteringcount=[point,point+windowize].count(clustering)
bestpoint=point
boundaries.append(point)
return boundaries

\ednote{end pseudocode}

This actually works surprisingly well, however it does need the extra parameter windowsize, and it doesn't work super well with data that has many different windowsizes, and has ugly combinations. Say $[1,1,2,1,2,2]$ and a windowsize of $3$, would have boundaries at 0, and at 2. Also has issues with the starting boundaries. Also it isn't theoretically satisfying. 

\ednote{obviously rephrase this.}
While the previous approach worked relatively well for "`nice"' \ednote{check if these sort of quotes work like I want them to} clusterings, with similar, known windowsizes and not much confusion, we also need an approach that works when the data gets more ugly.

The first clustering algorithm that almost everyone learns \ednote{I'm not sure this is a good transition} is k-means clustering \ednote{If I wnated to jsut cite this instead of explaining it, here would be a place to do so}. It would seem natural at first glance to use it to find the decision boundaries as well. However, it is made more complex by the fact that the data points that we would be applying to it to would be from $[1,\ldots,n_{cluster}]\times [1,\ldots,T]$, i.e. the points that we would want to cluster consist of a pair of clustering and point in time. However, the distance for the clusterings can't just be the standard distance function, since a clustering label of $1$ is in principle the same distance from a clustering label of $2$ as it is from a clustering label $10$.

Thus we define the distance function $d^*:(\mathbb{N}\times \mathbb{R})^2\rightarrow \mathbb{R}$ \ednote{check if this is properly formatted} by
\[
d^* ((c_1,t_1),(c_2,t_2))=\mu *\delta_{c_1,c_2} +|t_1-t_2|
\]
where $\delta_{i,j}$ is the Kronecker-delta. 

Thus we define the distance between two points to be the distance of their clustering and their temporal distance. We scale the distance of their clusterings by a factor $\mu > 0$, since $\delta_{i,j}$ is always either $1$ or $0$, while $t_i$ could range as high as $10000$ in some testcases. 

We choose a $\mu$ of about half the expected window size $w_{expected}$, as this will ensure that for some cluster with center $c^*$, every point in the interval $[c^*-w_{expected}/2,c^*+w_{expected}/2]$ with the same clustering as $c^*$ will be assigned to the $c^*$'s cluster. \ednote{More analysis can be done here on why this would be a cool choice and what the consequences would be, potentially some conditions as well}
\ednote{Still not entirely sure where/whether I should define k-means clustering}
Then the slightly modified $k-means$ we use is defined by
\ednote{again pseudo code here}
Initialize centers $c^*_i$
Initialize clusterings $C_i$empty.
While(not done)
{
for clustering,time in clusterlist:

$
{
i=argmin d^*((clustering,time),c^*_i)
C_i.append((clustering,time)
}
For all C_i:
c^*_i=(mostcommon(C_I[0]),mean(C_i[1])
repeat till happy$
}
\ednote{end pseudo code}
We note that this further departs from the conventional k-means algorithm by choosing not the average clustering as new center of a cluster, but instead the most common element. This is again a natural consequence of the nature of the clusterings as only integers.
\ednote{Needs to be rephrased. clustering versus cluster is awkward, I need to find a good name to call things}

After applying this technique, we are left with a list of clusters $C_i$ consisting of points $(old clustering label,time)$ \ednote{check if there are proper spaces and such in there} and their accompanying center $(c^*_i,t^*_i)$. To transform these into boundary points $b^*_i$ \ednote{check if this is how we called boundary points previously}, we simply project onto the temporal axis and are left with points $t^*_i$.

However these points correspond to centers of clusters, not decision boundaries. Naively, we could just take the middle point between two centers $t_i$ as decision boundary, however this does not deal well with situations such as $[1,1,1,1,2,2,2,2,2,2,2,2]$. Intuitively, the best centers for two clusters would be $(1,2)$ and $(2,8)$. However if we then took the center, we would get a decision boundary $(8+2)/2=5$, instead of the clearly superior $4$.

\ednote{is this good paragraph wise}
Thus we must find a more suitable way to get boundaries from centers than just taking their middle points respectively. \ednote{I don't like middlepoints, there should be a better word available} We recall, that we not only have the centers $(c^*_i,t^*_i)$, but also the clusters $C_i$ available. The problem arises when we try to find the boundary between two clusters of different size, thus it stands to reason that we can use the sizes of our clusters to better approxmimate a good boundary by using the formula:
\[
b_i=\frac{|C_i|t^*_{i+1}+|C_{i+1}t^*{i}}{|C_i|+|C_{i+1}|}
\]
If we apply this to our previous example, we get $b_i=\frac{4*2+8*5}{12}=4$, which is the result we wanted.

Thus, the second algorithm for finding boundaries is given by
\ednote{start pseudocode}
$Given list [c_1,\ldots,c_{T}] of clustering labels
C_i,c_i=modified kmeans (c_1,ldots,c_T)
For i in range(len(C_i)-1):
b_i=\frac{|C_i|*t^*{i+1}+|C_{i+1}|*t^*{i}}{|C_i|+|C_{i+1}|}$
\ednote{end pseudocode}
\ednote{Might be appropriate to put a small subsection in with diagram taht shows how the two clustering algorithms perform on different data}
\subsubsection{Number of Clusters}
Now that we have determined how to find boundaries, we must ask how many clusters are we looking for? Of course, if the number of desired clusters is known, we can simply look for the appropriate number of clusters.

However, if the appropriate number of clusters is unknown, we have to make some guess as to how many clusters to look for is appropriate \ednote{are appropriate? Not sure onthis one}. 

To answer this question we look at diagram \ednote{add in diagram here, that shows high amount of clusters versus low amount of clusters} . As we can see
, for a too low choice of boundaries, certain boundaries are missed completely, such as the one by ~1700.

If we choose a too high number of clusters, some of the boundaries seem to all correspond to the same boundary,as seen for the boundary by ~ 650 in the diagram, while some potentially finer clustering is achieved in different areas of the diagram. For the sample analyzed in the diagram, the first 250 timesteps consisted of a person walking, the different segmentations of the first real segment correspond to the person changing direction. 

Thus we conclude that if we are unsure on the amount of clusters/boundaries to look for, we should err on the side of caution and rather choose a too high number of boundaries than a too low one.

However we note that regardless of how many boundaries we choose to look for, the boundary at ~800 \ednote{is ~x even accepted? } is not detected by the algorithm. One potential cause of this is that similarities to off-diagonal elements in the similarity matrix are detected, that might interfere with the correct choice of boundary.

Hence we look at a possibly different choice of similarity matrix, to help alleviate the issue.
\subsubsection{Choice of Similarity matrix}
As discussed previously, the major choice in similarity matrix is choosing between the dtw-distance and euclidean distance, and further deciding how many off-diagonal elements to choose. 

In diagram \ednote{reference here}, we show five different choices of similarity matrix. The first choice is just using a full matrix filled with similarities computed using the euclidean distance, while the second is a full matrix of similarities computed using the dtw distance \ednote{check if dtw was capitalized before}. The last three matrices include are various hybrids, one with a stretch around the diagonal based on dtw-distance and the rest euclidean distance, and the other two with zeroes on the off-diagonal stretch and dtw/euclidean based similarities on the diagonal part respectively.

\ednote{include diagram of similarity matrices}

We use these matrices in on the same dataset as before, but with the correct number of clusters in diagram \ednote{reference here} . As we can see the two pure solution perform worst of all, while all three other solutions perform equally well. However we do not that for the hybrid dtw/euclidean matrix, while the found boundaries aligned about equally well, the first segment contains clustering labels from significantly later segments. Since our goal is to provide local boundaries, instead of trying to classify different motions, this leads us to conclude that taking only a band around the diagonal as nonzero is probably the best choice of similarity matrix. 

This is a very good result, since it means we probably don't have to compute the full matrices, and we especially don't have to compute the expensive full dtw matrix. 
\ednote{add diagram of clustering}



\subsubsection{Heuristics for better clustering}
Finally, we used several heuristics to improve the clustering result.

The heuristics can mainly be sorted into two groups: Improving the clustering result by modifying the clustering labels provided by spectral clustering, and trying to evaluate the quality of the implicit clustering provided by finding boundary points.

We initially constructed these heuristics when working on toydata, since we often got clustering results such as in Diagram X. \ednote{add diagram here. Two graphs 8 clusterrealdata, and DTWflawless} The two problems we focused on can be seen in  subdiagram 1, we have relatively clear regions such as $0-200$, with some outlier points at $80$ish, and in subdiagram at about $400$, where we have one clearly defined segment that is too short to be a segment all on its own. 

Our first heuristic was developped to try to deal with the outliers seen in subdiagram 1\ednote{check if references are working properly here}. We look at a window of a certain size centered around a point, and if more than $90\%$ \ednote{chekc if percent shows up here} of the window is one clustering label, we change that point to this most common clustering label. Since the window is centered around this point, we can be assured that we don't accidentally change the label at some actual transition point, since the lower half and the upper half of the interval would contain different clusterings.

\ednote{put pseudocode for this heuristic here}

With our second heuristic we tried to deal with the second problem, a clearly defined segment, that is too short to be a real segment. We assume that it actually belongs to one of the neighbouring segments, and we need to reattach it. However, we cannot know which of the two neighbouring segments it should belong to, just based on the list of clustering labels. Thus we compare the distance to the neighbouring segments in the similarity matrix, and attach the segment to the closer one.

Thus this heuristic consist of two parts, identifying the situation, one small segment surrounded by two bigger, different segments, and then finding out which of the two neighbouring segments to attach it to \ednote{decide if neighboring or neighbouring}.

\ednote{pseudocode for it here}
for point in list[windowsize/2:-windowsize/2]:
find mostcommon in list[point-windowsize/2:point+windowsize/2]
find secondmostcommon in list[point-windowsize/2:point+windowsize/2]
if list[point-windowsize/2:point+windowsize/2].count(point) < windowsize*0.2:
if count(mostcommon)>0.3*windowsize and count(secondmostcommon)> 0.3*windowsize: 
pointline=simmatrix[point]
highestclusterino=[i for i,j in enumerate(binarylist) if j==highest]
secondhighestclusterino=[i for i,j in enumerate(binarylist) if j==secondhighest]
\ednote{end pseudocode}

We note that for both of these heuristics, we set the thresholds for them to activate rather conservatively. At worst, the heuristics do not trigger, which leaves us in the same situation as before. If they triggered too often, it could lead to potential problems.

The heuristics to evaluate the quality of a clustering were developed when trying to answer the question of how many boundary points to use/into how many different clusters to cluster. The idea was that if we could cheaply figure out how good a clustering was, we could then reject reject a number of clusters that lead to bad clusterings.

Similarly to the previous two heuristics, we again have one heuristic that uses solely information from the clustering labels provided by spectral clustering, while the other one uses information from the similarity matrix. 

The first one simply uses the ratio of the number of occurrences of the most common clustering versus the size of the window, so if we have a cluster in the interval $[i,j]$, the score would be $[i,j].count(mostcommon)/(|i-j|)$. \ednote{is this formula formal enough? } Intuitively, a clustering that consists only of a single cluster would be optimal, a cluster that consisted of equally distributed all different clusterings would be the worst possible one.

The second heuristic considers that we already have similarity scores in the similarity matrix of choice, thus we can use them. If we again have a clustering implicitly provided by boundary points $i$ and $j$, we get a score of
\[
\sum\limits_{i,j} (S)_{i,j}/|i-j|^2
\]
where $S$ is the similarity matrix. 

This is then simply the average similarity in the clustering.

We can see some scores provided to clusterings in diagram \ednote{add diagram and reference it here}. As we can see, they both perform incredibly similarly, and give no real insight on whether a cluster is suitable or not. 
\ednote{We might want to put evaluation of clusterings in here.}
\ednote{Call this Batch SFA cutting or something, just find some nice name for the technique}

An alternative avenue of implicitly finding how many clusters is provided by our next approach.

\subsection{Mini Batch}
The main idea behind this approach is based on the fact that the most successful similarity matrices for the previous approach were the ones with $0$ everywhere but on a stretch around the diagonal. 

Similarly, we could look at a window of a certain fixed size, equivalent to the width of the non zero part around the diagonal, and then look for one boundary on this small window. We can then repeat this process, with a new window of the same size being considered, starting from the previous boundary. Eventually, the entire time sequence will be covered, at which point the algorithm will terminate.

As before, there are some key questions that need to be answered. First we have similar questions that we needed to answer for the previous algorithm, how do we choose boundaries, and what windowsize is appropriate. Additionally, there is a possible concern of propagating errors, if we find one boundary wrong, we will use an inappropriate window to find the next boundary, which might be wrong again, and so on and so forth.

We will answer these questions in the order that we stated them, as again, we need to know how to find boundaries to evaluate the results of different windowsizes, and we'd need to have appropriate windowsizes to answer how/whether errors propagate.

In contrast to the previous situation, where we had to find many boundaries, in this case we only need to find a single boundary. Thus we can simply try out every single boundary, and pick the best one, without it being prohibitively expensive. Thus for a given window $[i,i+windowisze]$, with a list of clustering labels $c_1,\ldots,c_{windowsize}$, we choose the boundary $b$ according to
\[
b=\argmin_{b} \sum\limits_{j=1}^{b} \delta_{0,c_j} +\sum\limits_{j=b+}^{windowsize} \delta_{1,c_j}
\]
We assume without loss of generality that the list of labels is ordered so that the average $0$ is earlier than the average $1$. Thus we are simply checking which list of $0$ until $b$ and $1$ afterwards is the closest to the original list of cluster labelings.
\ednote{As always, check for consistency of naming}

To answer the question about windowsizes, we first look at the question from a theoretical perspective. We consider some sample with segments each being about $w$ apart. What are the things we want to achieve with a window? We need to include the boundary that we want to find. We would like to not include any other boundary than the one we want to find. Finally we consider the amount of points available to find a boundary. If a boundary has many points on both sides of it, there are many points available to find it, thus we say it has much support. Thus we want the boundary we intend to find to have much support, while any unnecessary boundary should have less support.

In our theoretical inspection, we consider four different windowsizes, $w_1=1.5*w$, $w_2=2*w$, $w_3=2.5*w$, and $w_4=3*w$, and consider three different previous boundaries. The first previous boundary is $w/2$ behind the boundary it was intended to find, the previous boundary in the second situation is exactly on the boundary it was supposed to find, and the final previous boundary is $w/2$ ahead of where it was supposed to be. Thus the first and third previous situations have the maximum error in the previously found boundary, while the second one have the previous boundary found perfectly.

\ednote{Is this understandable?}
For the first situation, finding either the first boundary, the one that was previously undershot, or finding the second boundary, would be good outcomes, depending on whether our goal is not to miss any boundaries, or on finding all boundaries with the least amount of cutting points. If we use a windowsize of $w_1$, the window includes the previous boundary, and ends at the next boundary. If we use $w_2$, the window includes the first boundary, the second boundary and ends $w/2$ after the second boundary. If we use $w_3$, the window includes the first boundary, the second boundary, and ends at a third boundary. If we use $w_4$, the window includes the first, second and third boundary, and ends $w/2$ after the third boundary.

 Thus, for $w_1$, the included boundary is the one we had previously missed, and it has maximal support. For $w_2$, both the first and second boundary are included, with equal support. Hence, we would prefer $w_1$ to $w_2$, since it would be more consistent. For $w_3$, we include the first boundary and the second boundary, however the second boundary has a lot more support, thus it should normally find the second boundary. For $w_4$, we have a similar situation, except we also include the third boundary, but with little support.

Hence if we previously chose a boundary too early, we would prefer to use $w_1$ or $w_3$, over $w_2$ or $w_4$.

In the second proposed situation, the boundary we want to find is the first available one. If we choose a windowsize of $w_1$, it will include that boundary, with less support than if we have chosen $w_2$. If we choose $w_3$, we include the first boundary and the second boundary, however the support for the first boundary is higher than for the second boundary. If we choose $w_4$, we include the first and second boundary, with equal support for both. Thus in this situation we would prefer $w_2$ and $w_3$ over $w_1$, which we would prefer over $w_4$.

Finally, if we consider the situation where the previous boundary overshot the mark, we again have a similar situation to the first one, where we want to either find the first or second boundary, depending on preferences. Thus if we choose $w_1$, we include only the first boundary, if we choose $w_2$, we include the first and second boundary with equal support, if we choose $w_3$, we include the first and second boundary, with more support for the second boundary. Finally, if we choose a windowsize of $w_4$, we include the first, second, and third boundary, with equal support for the first and third boundary, but maximal support for the second boundary. Hence we would prefer windowsizes of $w_1$ or $w_3$, over $w_2$ or $w_4$.

If we consider all three situations equally likely, we conclude that choosing $w_3$, aka a slightly too large windowsize is the safest, since it will always include enough points that at least one boundary has full support. If we are sure that we never choose a boundary badly, choosing $w_2$ is the best bet. If we sometimes choose a boundary badly and want to make sure that every boundary is found, then $w_1$ is the best choice.

\ednote{Do the same conclusions but for different windowsizes. Afaik it was w_2 good on toydata with regularity, other better for irregular sizes, and w3 best for real data}

We can use the same diagrams to answer the question on whether/how errors propagate. \ednote{Actually get diagrams and do it, I don't really remember tbh}

\ednote{put in pseudocode for minibatch here}

We can see that for datasets with big differences in windowsizes, we often had trouble finding good boundaries \ednote{lko: Take a look at this again when you fill in the previous part.}, especially due to compounding errors. We tried several approaches to combat this.

One of the approaches we tried, which we called the doubletriple \ednote{find out if there's some cooler way, that's what I called it} , is based on the idea that the possible errors in finding boundaries is based on not having sufficient support to find it. Suppose we use two separate windowsizes $w_1$ and $w_2=w_1*1.5$, then we would expect to find one boundary in the first window and two boundaries in the second. If the one boundary found in $w_1$ has bad support, then it should have more support in $w_2$, thus if we look for two boundaries in $w_2$, we should find the boundary for $w_1$ as well. Since the actual boundary in $w_1$ would have had some support, we expect the boundary found by looking in $w_1$ to be close to the actual boundary, closer to it than to the second boundary found in $w_2$.

Thus we could use the boundaries $b_{11}$, $b_{21}$, and $b_{22}$ found in $w_1$ and $w_2$ as a committee. We expect the actual boundary $b^*$ in $w_1$ to be close to either $b_{21}$ or $b_{22}$ and we expect $b_{11}$ to be closer to $b^*$ than to the worse of $b_{21}$ and $b_{22}$. Thus our strategy becomes clear. We choose as boundary the center between $b_{11}$ and $b_{2}$, where $b_2=\argmin_{i} |b_{11}-b_{2i}|$. 

\ednote{pseudocode/algorithm for this here}
\ednote{Put diagram here and text explaining it. As far as I can remember, this should still be semi bad for some things.}

One problem that we faced for both tripledouble and standard minibatch, was that sometimes the optimal boundary found, was located at zero, the very start of the window. In the next step, we would consider the same window again, and come to the same conclusion again and so on and so forth. To combat this, we increased the windowsize by $20\%$ anytime we encountered a boundary at $0$, thus eventually increasing the window to large enough that a nonzero boundary was found, in which case we reverted to the same windowsize as before.

This points us in the direction of the next possible approach. Minibatch has the largest deficiency when dealing with strongly varying segment sizes or when we have a badly chosen default windowsize. If we were able to dynamically choose the windowsize, we would be able to get around these issues entirely. 

\subsection{Full Online Segmentation}
In this section we outline a possible way to make a fully online of our approach work.

As alluded to previously, the idea would essentially be to use minibatch, but with variable windowsizes. You would start with a candidate windowsize, evaluate whether it was a good windowsize, if yes, choose that windowsize, if no take a larger window, and repeat the process of evaluation. Once a windowsize is chosen, we would proceed similarly to minibatch, so first we'd find a boundary, and then proceed with a new window starting from that boundary.

To make this proposed algorithm work, the question that needs to be answered is how do we evaluate a window. What makes a window a good size for clustering?

Previously we had already decided that a windowsize that leads to a clustering with a boundary at $0$ is a bad windowsize. Potentially we could expand on this by considering a window good, if it leads to a good clustering/good boundaries.

There has been a lot of work done on the quality of clustering \ednote{Check the paper I have in reference papers for citations.}, however they've been focussed on evaluating the quality of a clustering on a given set of data versus other clusterings on the same data, not necessarily comparing two clusterings on expanded sets of data. 

We have previously tried our own metrics for clustering quality \ednote{put references to algorithms}, but they have shown to be unsuitable.

\ednote{We can do this discussion further I guess}
\ednote{possibly not gonna happen, remove subsection later if necessary}

If we manage to resolve the question how to evaluate the quality of a window, there is another opportunity to improve the algorithm. Since we potentially would propose and reject many windows in the run of using this, we would potentially have to do SFA on the same dataset many times over, as every new window by necessity includes the old window.

This means that using an online variant of SFA, such as \ednote{add citations to the two incsfa papers}, becomes possible. This would mean that we would just need to update the slow features with the new data, and not recompute the entirety from scratch.

Additionally, one feature of these incremental SFA algorithms is that the old feature values are unchanged, thus we would just have to compute a new row of the similarity matrix, again not having to recompute the entire thing from scratch.

Finally, incremental spectral clustering algorithms exist, \ednote{find citation for it} which allows us to do every step of our approach aside from finding the clusters themselves in an almost completely online fashion. 

\ednote{Try to make a shitty version of this work.}
\subsection{Parameters}
Reducenumber, number of features used. Delta and amount of features used for distance matrix. Distance measure (dtw versus just standard versus lazy as fuck :D )
